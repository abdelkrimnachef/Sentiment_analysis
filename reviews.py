# -*- coding: utf-8 -*-
"""review_text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18cB3nlUp8RxRU13kDdJYe_dw1je7gf7o
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import scale
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics  import accuracy_score,classification_report
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd
import numpy as np
import nltk
from nltk.stem import PorterStemmer
from sklearn.metrics import average_precision_score

from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from gensim.parsing.preprocessing import remove_stopwords
import sklearn as sk
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

Tfidv = TfidfVectorizer()
bow_vectorizer = CountVectorizer()

ps = PorterStemmer()
#df_short=fn.shrinker(1,1000,df)
  # np.set_printoptions(threshold=fn.np.inf)         #show all the array
  # pd_set_df_view_options(max_rows=10000, max_columns=20, display_width=320) #show all the dataframe
  # for pycharm only
from scipy.spatial.distance import cosine
from google.colab import drive
import matplotlib.pyplot as plt
# %matplotlib inline
drive.mount('/content/drive/')

def pd_set_df_view_options(max_rows, max_columns, display_width):

    # Show more than 10 or 20 rows when a dataframe comes back.
    pd.set_option('display.max_rows', max_rows)
    # Columns displayed in debug view
    pd.set_option('display.max_columns', max_columns)

    pd.set_option('display.width', display_width)
  # for pycharm only


#to print all dataframe
# /////////////////////////////////////////////////////////////////////////

def df_manip(df):
    for rev in range(0, len(df)): #itteration of the dataframe
        df.loc[rev, "phrase_len"]=len(df.loc[rev,"review"])
        df.loc[rev, "phrase_words"]=len(df.loc[rev,"review"].split())
        # df.loc[rev, "review"] = ", ".join(df.loc[rev,"review"].split())


    x=len(df)
    revi=0
    while revi < x: #itteration of the dataframe
         if  df.loc[revi, "phrase_words"]< 3: # lenght with phrase
             df=df.drop([revi])

         revi =revi+1

    return df
# to manipulate the df
# /////////////////////////////////////////////////////////////////////////

# to shrik a data set
def shrinker(start , end ,df):
    df = df[start:end]
    return df
# to shrik a data set




##########################################################################################
def stopW(df):
    x = len(df)
    revi = 0
    nltk.download('stopwords')     #i have to make it everey time i rebooor the server
    stop = set(stopwords.words('english'))

    while revi < x:  # itteration of the dataframe

        df.loc[revi, "review"] = stemSentence(df.loc[revi, "review"]) #call function to make steming
        df.loc[revi, "review"]= [i for i in df.loc[revi, "review"].split() if i not in stop] #na9sem f jomla split
        # symbols = "!\"#$%&()*+-../:;<=>?@[\]^'s_`'`-?!''.`'``{|},~\n"
        # df.loc[revi, "review"] = [i for i in df.loc[revi, "review"] if i not in symbols] #remove symbols (a revoire ay tmchi ms maych tnahi les --> ')
        revi=revi+1
    return df

# /////////////////////////////////////////////////////////////////////////

def punk(df):
    x = len(df)
    revi = 0

    while revi < x:  # itteration of the dataframe
        words = df.loc[revi, "review"]
        filtered_sentence = remove_stopwords(words)
        print(filtered_sentence)
        revi=revi+1
    return df
  # remove stop words
# /////////////////////////////////////////////////////////////////////////

def stemSentence(sentence):
    nltk.download('punkt')   #i have to make it everey time i rebooor the server
    token_words=word_tokenize(sentence)
    token_words
    stem_sentence=[]
    for word in token_words:
        stem_sentence.append(ps.stem(word))
        stem_sentence.append(" ")
    return "".join(stem_sentence)

def rem_two_chars(words):
    new_text = ""
    for w in words:
        if len(w) > 2:
            new_text = new_text + " " + w
    return new_text
def cosine_sim1(text1, text2):
    tfidf = Tfidv.fit_transform([text1, text2])
    return ((tfidf * tfidf.T).A)[0,1]
def cosine_sim2(text1, text2):
    bow = bow_vectorizer.fit_transform([text1, text2])
    return ((bow * bow.T).A)[0,1]
def data_mean(df):
  x = len(df)
  revi = 0
  negative=0
  positive=0
  nbr_pos=0
  nbr_neg=0
  while revi < x:  # itteration of the dataframe
    if(df.loc[revi, "sentiment"]=="positive"):
      nbr_pos=nbr_pos+1
      negative=negative + float(df.loc[revi, "phrase_words"])
      revi=revi+1

    if(df.loc[revi, "sentiment"]=="negative"):
      nbr_neg=nbr_neg+1
      positive=positive + float(df.loc[revi, "phrase_words"])
      revi=revi+1

  mean_pos=positive/nbr_pos
  mean_neg=negative/nbr_neg
  print('mean of positive is :', mean_pos , " \nmean of negative is: ", mean_neg)

print("we are using the Tf IdF")
df= pd.read_csv("/content/drive/MyDrive/Colab Notebooks/dataset/IMDB.csv" ,)  #to dataframe
# df= pd.read_csv("/content/drive/MyDrive/Colab Notebooks/dataset/IMccc.csv" ,)  #to dataframe

df.insert(2, "phrase_len", '') #add a column 
df.insert(3, "phrase_words", '') #add a column

df_new=df_manip(df)              #call fn to manipulate the Df to remove all pharess <= 3 and spllit
df_new = df_new.reset_index(drop=True)                  #reindex of the dtata frame
data_mean(df_new)
df_new # phrase len est cachÃ¨ (l affichage sur pycharm est complet :') je sais pas comment ca marche en collab

df_new=df_new.drop(['phrase_len'], axis = 1)            #remove th ecolumn
df_new=df_new.drop(['phrase_words'], axis = 1)          #remove th ecolumn
stat=df_new.sentiment.value_counts()                #statistics
print(stat)
print("*********************************************************")
df_new=stopW(df_new)
# ""###########################################################################
revi=0                                                   #remove all word sghar 3la 2 chars
while revi<len(df_new):                                 #remove all word sghar 3la 2 chars
  text=rem_two_chars(df_new.loc[revi,"review"])     #remove all word sghar 3la 2 chars
  df_new.loc[revi, "review"]=text                       #remove all word sghar 3la 2 chars
  revi=revi+1                                              #remove all word sghar 3la 2 chars
# ##############################################################################
DF = {}
for i in range(len(df_new["review"])):                                  #term
  tokens = str(df_new.loc[i,"review"]).split()                        #term
  for w in tokens:                                                        #term
    try:                                                                #term
        DF[w].add(i)                                                    #term
    except:                                                             #term
        DF[w] = {i}                                                      #term
# for i in DF:                                                                #term ferquency
#     DF[i]=len(DF[i])                                                        #term ferquency
# print(DF)
vocab=[x for x in DF]                                                           #get vocabulary
print("Vocabulary is-----> ",vocab)    #get vocabulary
print("*********************************************")
table=df_new["review"]                                                  #put vocabulary in an array
table2=df_new["review"]                                                  #put vocabulary in an array
table3=df_new["review"]                                                  #put vocabulary in an array

df_new["sentiment"]=df_new["sentiment"].map({'positive':1,'negative':0})
df_next=df_new
df_new1=df_new
df_new

df.plot(x ='sentiment', y='phrase_len', kind = 'line')
plt.show()

sparce_mat = Tfidv.fit_transform(table)
save=sparce_mat
print(sparce_mat)

X_train, X_test, y_train, y_test = train_test_split(sparce_mat, df_new["sentiment"], test_size=0.3)
classifier = KNeighborsClassifier(n_neighbors=7)
classifier.fit(X_train,y_train)
y_pred=classifier.predict(X_test)
classification_report=classification_report(y_test,y_pred)
print(classification_report)
print(confusion_matrix(y_test,y_pred))

sparce_mat1 = bow_vectorizer.fit_transform(table)
save=sparce_mat1
print(sparce_mat1)

X1_train, X1_test, y1_train, y1_test = train_test_split(sparce_mat1, df_new["sentiment"], test_size=0.3)

classifier = KNeighborsClassifier(n_neighbors=7)
classifier.fit(X1_train,y1_train)
y1_pred=classifier.predict(X1_test)
from sklearn.metrics  import accuracy_score,classification_report

classification_report1=classification_report(y1_test,y1_pred)
print(classification_report1)
print(confusion_matrix(y1_test,y1_pred))

sentense =input("\nwrite your sentense \n")
df_s = pd.DataFrame(columns=['review','phrase_len','phrase_words'])
df_s.loc[0,"review"]= sentense
df_s = df_manip(df_s)
df_s=stopW(df_s)
revi=0                                                   #remove all word sghar 3la 2 chars
while revi<len(df_s):                                 #remove all word sghar 3la 2 chars
  text=rem_two_chars(df_s.loc[revi,"review"])     #remove all word sghar 3la 2 chars
  df_s.loc[revi, "review"]=text                       #remove all word sghar 3la 2 chars
  revi=revi+1
df_s=df_s.drop(['phrase_len'], axis = 1)            #remove th ecolumn
df_s=df_s.drop(['phrase_words'], axis = 1)   
df_new["sim"]= ""
# print(df_new)
df_new2=df_new
df_new3=df_new
df_s

i=0
while i<len(table2):

  df_new2.loc[i,'sim']=cosine_sim2(df_new2.loc[i,'review'], df_s.loc[0,'review'])
  i=i+1
df_new2=df_new2.sort_values(by=['sim'])
df_new2=df_new2.drop(['sim'], axis = 1)          #remove th ecolumn  
df_new2=df_new2.drop(['sentiment'], axis = 1)          #remove th ecolumn  
print("using BOW 5 similaire sentensces")
print(df_new2.tail(5))

i=0
tfidf_vectorizer = TfidfVectorizer()
print("using TFIDF 5 similaire sentensces")

while i<len(table3):
    
  tfidf_1= tfidf_vectorizer.fit_transform([df_new3.loc[i,'review']])
  tfidf_2= tfidf_vectorizer.transform([df_s.loc[0,'review']])

  df_new3.loc[i,'sim']=cosine_similarity(tfidf_1,tfidf_2)
  # df_new.loc[i,'sim']=cosine_sim2(df_new.loc[i,'review'], df_s.loc[0,'review'])
  i=i+1
df_new3=df_new3.sort_values(by=['sim'])
df_new3=df_new3.drop(['sim'], axis = 1)          #remove th ecolumn  
df_new3=df_new3.drop(['sentiment'], axis = 1)          #remove th ecolumn  
print(df_new3.tail(5))

print("we can say that the tf idf is better then the bow whene we see the result and the score the precision and  the recall are little then the tfidf and if the recal and the precision are low means that the model makes lot of false nefative and positive \n Tfidf make a word ponderation from all the documents or all sentences but the bow makes a frequency of the the world only    ")

"""# New section"""